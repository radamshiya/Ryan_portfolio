[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.8","content-config-digest","f145393ce5ae6e51","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://yourusername.github.io\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,48,49,73,74,98,99],"04-analytics",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"rendered":23,"legacyId":47},{"title":14,"summary":15,"role":16,"date":17,"image":18,"featured":19},"Redesigning Analytics for Accuracy at Scale","We fixed broken call center analytics through research, analysis, and design before customers felt the impact.","Lead Researcher","2024","/images/hiya_connect.png",true,"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nHiya Connect's call center analytics were increasingly unreliable. Different data sources measured calls in incompatible ways, making metrics inconsistent and difficult to interpret. \n\u003Cbr>\u003Cbr>\nAt the same time, new calling features like Apple’s Visual Voicemail were causing traditional metrics like answer rate to appear artificially inflated. \n\u003Cbr>\u003Cbr>\nWe knew customers were making decisions based on data that no longer reflected reality, and upcoming platform changes would only make this worse.\n\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Delivered a globally consistent analytics framework with no regional caveats\n- Ensured analytics were accurate, comparable, and consistently measured over time\n- Eliminated scenarios where customers would see misleading or unusable data\n- Reduced reliance on fragile third-party data sources and long-term platform risk\n- Launched with minimal customer support issues or post-launch confusion\n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI partnered closely with the product manager to define the problem and project plan. I led all discovery, customer research, and statistical analysis, working with data scientists to validate findings. I translated customer needs into design inputs, led usability research on the analytics experience, and partnered with product marketing on customer and internal education around sampling and why this approach was superior to alternatives.\n\n## Approach\n\nWe began by speaking with internal subject matter experts and customers to understand how analytics were actually being used day to day. This work focused on the decisions customers were trying to make, the problems they relied on analytics to solve, and where existing metrics were already falling short. These insights shaped how we approached the redesign from the start.\n\u003Cbr>\u003Cbr>\nFrom there, we audited every available data source to assess coverage, consistency, and long-term viability. I led an evaluation of which sources provided sufficient volume to support statistically meaningful insights across customers.\n\u003Cbr>\u003Cbr>\nPartnering closely with data science, we built a model to determine minimum data thresholds for statistical significance and validated those thresholds against real-world customer data. This led us to a sampling-based analytics approach that balanced accuracy, scalability, and resilience to platform changes.\n\u003Cbr>\u003Cbr>\nI worked with design to ensure the analytics experience never positioned data as more precise than it actually was. When statistical confidence was limited, the product communicated that clearly. We tested these concepts internally and externally, including an independent evaluation with a University of Washington student research team.\n\n## Findings\n\n- Grounding analytics work in real customer decision-making leads to stronger, more resilient systems\n- Accuracy and consistency build trust faster than more metrics or increased complexity\n- Statistical concepts are most effective when available but not forced into the primary experience\n- Early cross-functional alignment and education dramatically reduce launch risk and downstream support needs","src/content/projects/04-analytics.md","8351efb98de8d276",{"html":24,"metadata":25},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>Hiya Connect’s call center analytics were increasingly unreliable. Different data sources measured calls in incompatible ways, making metrics inconsistent and difficult to interpret.\n\u003Cbr>\u003Cbr>\nAt the same time, new calling features like Apple’s Visual Voicemail were causing traditional metrics like answer rate to appear artificially inflated.\n\u003Cbr>\u003Cbr>\nWe knew customers were making decisions based on data that no longer reflected reality, and upcoming platform changes would only make this worse.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Delivered a globally consistent analytics framework with no regional caveats\u003C/li>\n\u003Cli>Ensured analytics were accurate, comparable, and consistently measured over time\u003C/li>\n\u003Cli>Eliminated scenarios where customers would see misleading or unusable data\u003C/li>\n\u003Cli>Reduced reliance on fragile third-party data sources and long-term platform risk\u003C/li>\n\u003Cli>Launched with minimal customer support issues or post-launch confusion\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I partnered closely with the product manager to define the problem and project plan. I led all discovery, customer research, and statistical analysis, working with data scientists to validate findings. I translated customer needs into design inputs, led usability research on the analytics experience, and partnered with product marketing on customer and internal education around sampling and why this approach was superior to alternatives.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>We began by speaking with internal subject matter experts and customers to understand how analytics were actually being used day to day. This work focused on the decisions customers were trying to make, the problems they relied on analytics to solve, and where existing metrics were already falling short. These insights shaped how we approached the redesign from the start.\n\u003Cbr>\u003Cbr>\nFrom there, we audited every available data source to assess coverage, consistency, and long-term viability. I led an evaluation of which sources provided sufficient volume to support statistically meaningful insights across customers.\n\u003Cbr>\u003Cbr>\nPartnering closely with data science, we built a model to determine minimum data thresholds for statistical significance and validated those thresholds against real-world customer data. This led us to a sampling-based analytics approach that balanced accuracy, scalability, and resilience to platform changes.\n\u003Cbr>\u003Cbr>\nI worked with design to ensure the analytics experience never positioned data as more precise than it actually was. When statistical confidence was limited, the product communicated that clearly. We tested these concepts internally and externally, including an independent evaluation with a University of Washington student research team.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Grounding analytics work in real customer decision-making leads to stronger, more resilient systems\u003C/li>\n\u003Cli>Accuracy and consistency build trust faster than more metrics or increased complexity\u003C/li>\n\u003Cli>Statistical concepts are most effective when available but not forced into the primary experience\u003C/li>\n\u003Cli>Early cross-functional alignment and education dramatically reduce launch risk and downstream support needs\u003C/li>\n\u003C/ul>",{"headings":26,"localImagePaths":43,"remoteImagePaths":44,"frontmatter":45,"imagePaths":46},[27,31,34,37,40],{"depth":28,"slug":29,"text":30},2,"problem","Problem",{"depth":28,"slug":32,"text":33},"outcomes","Outcomes",{"depth":28,"slug":35,"text":36},"my-role","My Role",{"depth":28,"slug":38,"text":39},"approach","Approach",{"depth":28,"slug":41,"text":42},"findings","Findings",[],[],{"title":14,"summary":15,"role":16,"date":17,"image":18,"featured":19},[],"04-analytics.md","02-machines",{"id":48,"data":50,"body":56,"filePath":57,"digest":58,"rendered":59,"legacyId":72},{"title":51,"summary":52,"role":53,"date":54,"image":55,"featured":19},"How Machines Should Talk","Creating a design system for conversations applicable to new zero-to-one products.","Research & Strategy","2024–2025","/images/machines_talking.png","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nHiya saw a clear opportunity to explore AI voice agents. Voice calls are the core of our business, and generative AI presented a path to both solve existing customer problems in the voice channel and expand into adjacent problems we were not yet addressing.\n\u003Cbr>\u003Cbr>\nThis was a completely new space for the company. Not just in terms of new technology, but in understanding *how* AI voice agents should behave. The industry conversation was dominated by demos that optimized for sounding human or appearing impressive, often framed around concepts like the uncanny valley. What was missing was a grounded understanding of what businesses actually want AI agents to sound like, how they want those agents to represent them, and what people calling or using those systems actually value in real interactions.\n\u003Cbr>\u003Cbr>\nWe needed a durable framework that could guide design decisions as the technology rapidly evolved, rather than chasing short-term trends or surface-level feedback.\n\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Created a living conversational design system applicable across teams and products  \n- Successfully launched two zero-to-one products conversational products, Hiya AI Phone and Hiya Voice Agents.\n- Strong qualitative feedback on usability, trust, and engagement with AI voice experiences  \n- Balanced regulatory requirements (such as call recording disclosures across locales) with positive user experience  \n- Built internal confidence that we had the right combination of technologies to deliver the desired experience  \n- Enabled teams to stay focused despite rapid changes in the underlying AI technology landscape  \n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI led the overall strategy and research, framing why conversational design needed to prioritize real user needs over what demos well. I drove all research, synthesis, and decision-making around how AI voice agents should behave and how those principles should be applied across products.\n\n## Approach\n\nThis work spanned a wide range of activities due to the rapidly evolving nature of the technology. We conducted ongoing technical exploration to evaluate available AI voice technologies, continuously testing their strengths, limitations, and changes over time. Because the landscape shifted quickly, this required repeated iteration rather than one-time evaluation.\n\u003Cbr>\u003Cbr>\nI designed surveys and controlled laboratory studies to test different voices, accents, pacing, and latency configurations. In partnership with engineering and product interns working on translation services, we built test environments that allowed native speakers to evaluate multilingual voice interactions and translation quality.\n\u003Cbr>\u003Cbr>\nWe partnered twice with the University of Washington to run applied research on how people want to interact with AI systems. These collaborations included both behavioral research and speculative prototyping to explore future interaction models.\n\u003Cbr>\u003Cbr>\nBeyond structured research, we developed interactive prototypes outside of our core products and, in some cases, deployed them into production to gather real-world feedback. This helped mitigate the artificiality of lab settings, where people often behave differently when they know they are interacting with AI.\n\u003Cbr>\u003Cbr>\nWe also conducted extensive secondary research, drawing from studies on human conversation, cognitive load, turn-taking, interruption, and information retention to better understand constraints like how much information can be communicated effectively during a voice interaction.\n\n## Findings\n\n- What matters most in AI voice interactions is not realism, but clarity, predictability, and trust  \n- Latency and conversational flow have a greater impact on experience than accent or vocal personality  \n- Users value agents that represent them appropriately over agents that attempt to sound human  \n- Providing consistent conversational rules creates better experiences than optimizing each interaction independently  \n- A strong design system allows teams to evaluate feedback in context, distinguish outliers from systemic issues, and resist reacting to every new technology shift","src/content/projects/02-machines.md","ec78a2496bc84d14",{"html":60,"metadata":61},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>Hiya saw a clear opportunity to explore AI voice agents. Voice calls are the core of our business, and generative AI presented a path to both solve existing customer problems in the voice channel and expand into adjacent problems we were not yet addressing.\n\u003Cbr>\u003Cbr>\nThis was a completely new space for the company. Not just in terms of new technology, but in understanding \u003Cem>how\u003C/em> AI voice agents should behave. The industry conversation was dominated by demos that optimized for sounding human or appearing impressive, often framed around concepts like the uncanny valley. What was missing was a grounded understanding of what businesses actually want AI agents to sound like, how they want those agents to represent them, and what people calling or using those systems actually value in real interactions.\n\u003Cbr>\u003Cbr>\nWe needed a durable framework that could guide design decisions as the technology rapidly evolved, rather than chasing short-term trends or surface-level feedback.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Created a living conversational design system applicable across teams and products\u003C/li>\n\u003Cli>Successfully launched two zero-to-one products conversational products, Hiya AI Phone and Hiya Voice Agents.\u003C/li>\n\u003Cli>Strong qualitative feedback on usability, trust, and engagement with AI voice experiences\u003C/li>\n\u003Cli>Balanced regulatory requirements (such as call recording disclosures across locales) with positive user experience\u003C/li>\n\u003Cli>Built internal confidence that we had the right combination of technologies to deliver the desired experience\u003C/li>\n\u003Cli>Enabled teams to stay focused despite rapid changes in the underlying AI technology landscape\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I led the overall strategy and research, framing why conversational design needed to prioritize real user needs over what demos well. I drove all research, synthesis, and decision-making around how AI voice agents should behave and how those principles should be applied across products.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>This work spanned a wide range of activities due to the rapidly evolving nature of the technology. We conducted ongoing technical exploration to evaluate available AI voice technologies, continuously testing their strengths, limitations, and changes over time. Because the landscape shifted quickly, this required repeated iteration rather than one-time evaluation.\n\u003Cbr>\u003Cbr>\nI designed surveys and controlled laboratory studies to test different voices, accents, pacing, and latency configurations. In partnership with engineering and product interns working on translation services, we built test environments that allowed native speakers to evaluate multilingual voice interactions and translation quality.\n\u003Cbr>\u003Cbr>\nWe partnered twice with the University of Washington to run applied research on how people want to interact with AI systems. These collaborations included both behavioral research and speculative prototyping to explore future interaction models.\n\u003Cbr>\u003Cbr>\nBeyond structured research, we developed interactive prototypes outside of our core products and, in some cases, deployed them into production to gather real-world feedback. This helped mitigate the artificiality of lab settings, where people often behave differently when they know they are interacting with AI.\n\u003Cbr>\u003Cbr>\nWe also conducted extensive secondary research, drawing from studies on human conversation, cognitive load, turn-taking, interruption, and information retention to better understand constraints like how much information can be communicated effectively during a voice interaction.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>What matters most in AI voice interactions is not realism, but clarity, predictability, and trust\u003C/li>\n\u003Cli>Latency and conversational flow have a greater impact on experience than accent or vocal personality\u003C/li>\n\u003Cli>Users value agents that represent them appropriately over agents that attempt to sound human\u003C/li>\n\u003Cli>Providing consistent conversational rules creates better experiences than optimizing each interaction independently\u003C/li>\n\u003Cli>A strong design system allows teams to evaluate feedback in context, distinguish outliers from systemic issues, and resist reacting to every new technology shift\u003C/li>\n\u003C/ul>",{"headings":62,"localImagePaths":68,"remoteImagePaths":69,"frontmatter":70,"imagePaths":71},[63,64,65,66,67],{"depth":28,"slug":29,"text":30},{"depth":28,"slug":32,"text":33},{"depth":28,"slug":35,"text":36},{"depth":28,"slug":38,"text":39},{"depth":28,"slug":41,"text":42},[],[],{"title":51,"summary":52,"role":53,"date":54,"image":55,"featured":19},[],"02-machines.md","01-workshop",{"id":73,"data":75,"body":81,"filePath":82,"digest":83,"rendered":84,"legacyId":97},{"title":76,"summary":77,"role":78,"date":79,"image":80,"featured":19},"Shaping the Future of AI Within Research and Design","Crafting AI workshops for the University of Washington’s Master of Human-Computer Interaction and Design program.","Co-Instructor and Curriculum Designer","2025","/images/ryan_sarah_workshop.png","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nThe University of Washington’s MHCI+D program felt it was falling behind in preparing students to work with real-world applications of generative AI. While AI dominated the news cycle, students lacked clear, practical frameworks for understanding how these tools were actually being designed, evaluated, and used in industry.\n\u003Cbr>\u003Cbr>\nBased on my prior sponsorship and mentorship work with the program, as well as my hands-on experience leading generative AI initiatives at Hiya, the university approached me to develop a workshop series to address this gap. I partnered with Sarah Outhwaite, a designer at Google working on generative AI products, to co-create and deliver the curriculum.\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Workshop series adopted by the university as a full elective starting in 2026  \n- Strong attendance and sustained student engagement across sessions  \n- Reusable curriculum applied to internal teams at multiple companies  \n- Led to additional speaking engagements and workshops at other universities  \n- Established an ongoing feedback loop with students that refined teaching methods and perspectives on applied AI  \n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI served as co-instructor and curriculum designer, partnering closely with Sarah Othwaite to define the workshop structure, develop content and exercises, design hands-on activities, and deliver all sessions.\n\n## Approach\n\nWe designed a three-part workshop series, with each session lasting three hours and delivered week over week. Ahead of the workshops, we spent roughly six weeks aligning on perspective and defining a clear foundation for teaching generative AI.\n\u003Cbr>\u003Cbr>\nGiven how broad and hype-driven the AI landscape had become, we intentionally scoped the curriculum to focus on generative AI as it is actually built and used in practice. We addressed media narratives directly, contrasted them with real adoption patterns, and used historical technology shifts to contextualize how unusually fast AI adoption has been.\n\u003Cbr>\u003Cbr>\nOnce that foundation was established, the workshops moved into practical application, including designing for AI, using AI within the design process, prompt design, usability testing, risk management, and the limitations of analogy and metaphor when describing AI systems.\n\u003Cbr>\u003Cbr>\nSessions were intentionally hands-on, prioritizing experimentation, discussion, and learning through practice rather than prescribing a single “right” way to work with AI.\n\n## Findings\n\n- Despite an abundance of online content, many people struggle to navigate AI due to distorted media narratives  \n- Students and practitioners often need permission to experiment, explore, and be wrong more than instruction \n- Hands-on exposure rapidly builds confidence, even for participants with no prior AI experience  \n- Treating AI as a design material rather than a feature or tool resonates strongly with researchers and designers","src/content/projects/01-workshop.md","80466448b87178e0",{"html":85,"metadata":86},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>The University of Washington’s MHCI+D program felt it was falling behind in preparing students to work with real-world applications of generative AI. While AI dominated the news cycle, students lacked clear, practical frameworks for understanding how these tools were actually being designed, evaluated, and used in industry.\n\u003Cbr>\u003Cbr>\nBased on my prior sponsorship and mentorship work with the program, as well as my hands-on experience leading generative AI initiatives at Hiya, the university approached me to develop a workshop series to address this gap. I partnered with Sarah Outhwaite, a designer at Google working on generative AI products, to co-create and deliver the curriculum.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Workshop series adopted by the university as a full elective starting in 2026\u003C/li>\n\u003Cli>Strong attendance and sustained student engagement across sessions\u003C/li>\n\u003Cli>Reusable curriculum applied to internal teams at multiple companies\u003C/li>\n\u003Cli>Led to additional speaking engagements and workshops at other universities\u003C/li>\n\u003Cli>Established an ongoing feedback loop with students that refined teaching methods and perspectives on applied AI\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I served as co-instructor and curriculum designer, partnering closely with Sarah Othwaite to define the workshop structure, develop content and exercises, design hands-on activities, and deliver all sessions.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>We designed a three-part workshop series, with each session lasting three hours and delivered week over week. Ahead of the workshops, we spent roughly six weeks aligning on perspective and defining a clear foundation for teaching generative AI.\n\u003Cbr>\u003Cbr>\nGiven how broad and hype-driven the AI landscape had become, we intentionally scoped the curriculum to focus on generative AI as it is actually built and used in practice. We addressed media narratives directly, contrasted them with real adoption patterns, and used historical technology shifts to contextualize how unusually fast AI adoption has been.\n\u003Cbr>\u003Cbr>\nOnce that foundation was established, the workshops moved into practical application, including designing for AI, using AI within the design process, prompt design, usability testing, risk management, and the limitations of analogy and metaphor when describing AI systems.\n\u003Cbr>\u003Cbr>\nSessions were intentionally hands-on, prioritizing experimentation, discussion, and learning through practice rather than prescribing a single “right” way to work with AI.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Despite an abundance of online content, many people struggle to navigate AI due to distorted media narratives\u003C/li>\n\u003Cli>Students and practitioners often need permission to experiment, explore, and be wrong more than instruction\u003C/li>\n\u003Cli>Hands-on exposure rapidly builds confidence, even for participants with no prior AI experience\u003C/li>\n\u003Cli>Treating AI as a design material rather than a feature or tool resonates strongly with researchers and designers\u003C/li>\n\u003C/ul>",{"headings":87,"localImagePaths":93,"remoteImagePaths":94,"frontmatter":95,"imagePaths":96},[88,89,90,91,92],{"depth":28,"slug":29,"text":30},{"depth":28,"slug":32,"text":33},{"depth":28,"slug":35,"text":36},{"depth":28,"slug":38,"text":39},{"depth":28,"slug":41,"text":42},[],[],{"title":76,"summary":77,"role":78,"date":79,"image":80,"featured":19},[],"01-workshop.md","03-aquistion",{"id":98,"data":100,"body":105,"filePath":106,"digest":107,"rendered":108,"legacyId":121},{"title":101,"summary":102,"role":16,"date":103,"image":104,"featured":19},"10,000ft Acquisition and Integration","Crafting a strategy for integrating 10,000ft and Smartsheet.","2019","/images/10kft_smartsheet.png","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nAfter four years as an early employee at 10,000ft, the company was acquired by Smartsheet. Smartsheet wanted to integrate the two products immediately, but there was no clear strategy for how the products should work together or what the integration should enable for customers.\n\u003Cbr>\u003Cbr>\nComplicating this further, there was no obvious shared object between the two platforms that made integration straightforward. Smartsheet and 10,000ft were often purchased by the same companies, but rarely used by the same teams—or even the same people. We needed to quickly understand how customers actually used both tools, where integration would add value, and where it could introduce unnecessary complexity.\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Successfully launched the initial 10,000ft–Smartsheet product integration\n- Drived meaningful post-acquisition revenue growth across the broader Smartsheet organization\n- Enabled sales teams to confidently sell resource management alongside work management\n- Clarified how and when customers should use each product together without redundancy\n- Established a shared internal understanding of customer workflows across both platforms\n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI partnered with the product director to define the research strategy, led all research operations and customer discovery, synthesized and shared insights across Smartsheet, worked closely with design to define integration constraints and user needs, and led extensive usability testing prior to launch.\n\n## Approach\n\nWe began by deeply learning the Smartsheet product to understand its mental models, constraints, and value propositions. We then evaluated customer data to identify organizations using both platforms and worked to locate individuals who had exposure to both, which turned out to be relatively rare.\n\u003Cbr>\u003Cbr>\nTo reach these users, I identified geographic concentrations in areas like New York City and San Francisco and organized an in-person research roadshow. At each customer site (primarily design and advertising consultancies)we spent two to three hours with key stakeholders conducting detailed process-mapping sessions. We mapped how projects moved from inception to completion, where Smartsheet and 10,000ft fit into that flow, which tool projects originated in, and where information overlapped or diverged.\n\u003Cbr>\u003Cbr>\nAlongside this, we worked with design and engineering to explore integration concepts and constraints. I partnered closely with the designer, running weekly usability studies to test prototypes and iterate quickly as solutions evolved.\n\u003Cbr>\u003Cbr>\nWe also conducted a broad internal education push, including an all-hands presentation I led for Smartsheet, sharing research findings and framing how customers actually thought about work, resourcing, and integration.\n\n## Findings\n\n- Resource management is closer to casting than staffing—customers prioritize nuanced skill sets and experience over roles or titles\n- Customers sometimes intentionally keep tools out of sync, making control and flexibility more valuable than automation\n- There was no single “right” direction for integration; customers needed the ability to move data both ways depending on where work originated\n- Clear articulation of how two products complement each other is critical for successful cross-selling after an acquisition","src/content/projects/03-aquistion.md","87b6527b62262c71",{"html":109,"metadata":110},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>After four years as an early employee at 10,000ft, the company was acquired by Smartsheet. Smartsheet wanted to integrate the two products immediately, but there was no clear strategy for how the products should work together or what the integration should enable for customers.\n\u003Cbr>\u003Cbr>\nComplicating this further, there was no obvious shared object between the two platforms that made integration straightforward. Smartsheet and 10,000ft were often purchased by the same companies, but rarely used by the same teams—or even the same people. We needed to quickly understand how customers actually used both tools, where integration would add value, and where it could introduce unnecessary complexity.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Successfully launched the initial 10,000ft–Smartsheet product integration\u003C/li>\n\u003Cli>Drived meaningful post-acquisition revenue growth across the broader Smartsheet organization\u003C/li>\n\u003Cli>Enabled sales teams to confidently sell resource management alongside work management\u003C/li>\n\u003Cli>Clarified how and when customers should use each product together without redundancy\u003C/li>\n\u003Cli>Established a shared internal understanding of customer workflows across both platforms\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I partnered with the product director to define the research strategy, led all research operations and customer discovery, synthesized and shared insights across Smartsheet, worked closely with design to define integration constraints and user needs, and led extensive usability testing prior to launch.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>We began by deeply learning the Smartsheet product to understand its mental models, constraints, and value propositions. We then evaluated customer data to identify organizations using both platforms and worked to locate individuals who had exposure to both, which turned out to be relatively rare.\n\u003Cbr>\u003Cbr>\nTo reach these users, I identified geographic concentrations in areas like New York City and San Francisco and organized an in-person research roadshow. At each customer site (primarily design and advertising consultancies)we spent two to three hours with key stakeholders conducting detailed process-mapping sessions. We mapped how projects moved from inception to completion, where Smartsheet and 10,000ft fit into that flow, which tool projects originated in, and where information overlapped or diverged.\n\u003Cbr>\u003Cbr>\nAlongside this, we worked with design and engineering to explore integration concepts and constraints. I partnered closely with the designer, running weekly usability studies to test prototypes and iterate quickly as solutions evolved.\n\u003Cbr>\u003Cbr>\nWe also conducted a broad internal education push, including an all-hands presentation I led for Smartsheet, sharing research findings and framing how customers actually thought about work, resourcing, and integration.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Resource management is closer to casting than staffing—customers prioritize nuanced skill sets and experience over roles or titles\u003C/li>\n\u003Cli>Customers sometimes intentionally keep tools out of sync, making control and flexibility more valuable than automation\u003C/li>\n\u003Cli>There was no single “right” direction for integration; customers needed the ability to move data both ways depending on where work originated\u003C/li>\n\u003Cli>Clear articulation of how two products complement each other is critical for successful cross-selling after an acquisition\u003C/li>\n\u003C/ul>",{"headings":111,"localImagePaths":117,"remoteImagePaths":118,"frontmatter":119,"imagePaths":120},[112,113,114,115,116],{"depth":28,"slug":29,"text":30},{"depth":28,"slug":32,"text":33},{"depth":28,"slug":35,"text":36},{"depth":28,"slug":38,"text":39},{"depth":28,"slug":41,"text":42},[],[],{"title":101,"summary":102,"role":16,"date":103,"image":104,"featured":19},[],"03-aquistion.md"]