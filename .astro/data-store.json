[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.8","content-config-digest","1c5c441a87d32b58","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://radamshiya.github.io\",\"compressHTML\":true,\"base\":\"/Ryan_portfolio/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","projects",["Map",11,12,50,51,77,78,104,105,130,131],"01-workshop",{"id":11,"data":13,"body":22,"filePath":23,"digest":24,"rendered":25,"legacyId":49},{"title":14,"summary":15,"role":16,"date":17,"image":18,"imageAlt":19,"imageCaption":20,"featured":21},"Shaping the Future of AI Within Research and Design","Crafting AI workshops for the University of Washington's Master of Human-Computer Interaction and Design program.","Co-Instructor and Curriculum Designer","2025","/images/ryan_sarah_workshop.png","Ryan Adams and Sarah Outhwaite presenting an AI workshop to students at the University of Washington","Co-instructors Ryan Adams and Sarah Outhwaite leading a hands-on AI workshop session for MHCI+D students",true,"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nThe University of Washington’s MHCI+D program felt it was falling behind in preparing students to work with real-world applications of generative AI. While AI dominated the news cycle, students lacked clear, practical frameworks for understanding how these tools were actually being designed, evaluated, and used in industry.\n\u003Cbr>\u003Cbr>\nBased on my prior sponsorship and mentorship work with the program, as well as my hands-on experience leading generative AI initiatives at Hiya, the university approached me to develop a workshop series to address this gap. I partnered with Sarah Outhwaite, a designer at Google working on generative AI products, to co-create and deliver the curriculum.\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Workshop series adopted by the university as a full elective starting in 2026  \n- Strong attendance and sustained student engagement across sessions  \n- Reusable curriculum applied to internal teams at multiple companies  \n- Led to additional speaking engagements and workshops at other universities  \n- Established an ongoing feedback loop with students that refined teaching methods and perspectives on applied AI  \n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI served as co-instructor and curriculum designer, partnering closely with Sarah Othwaite to define the workshop structure, develop content and exercises, design hands-on activities, and deliver all sessions.\n\n## Approach\n\n\u003Cfigure class=\"float-right\">\n  \u003Cimg src=\"/images/ryan_presenting.jpg\" alt=\"Ryan presenting at UW in front of a slide that reads 'Assessing Risk'\" />\n  \u003Cfigcaption>Presenting during AI Workshops in 2025\u003C/figcaption>\n\u003C/figure>\n\nWe designed a three-part workshop series, with each session lasting three hours and delivered week over week. Ahead of the workshops, we spent roughly six weeks aligning on perspective and defining a clear foundation for teaching generative AI.\n\nGiven how broad and hype-driven the AI landscape had become, we intentionally scoped the curriculum to focus on generative AI as it is actually built and used in practice. We addressed media narratives directly, contrasted them with real adoption patterns, and used historical technology shifts to contextualize how unusually fast AI adoption has been.\n\nOnce that foundation was established, the workshops moved into practical application, including designing for AI, using AI within the design process, prompt design, usability testing, risk management, and the limitations of analogy and metaphor when describing AI systems.\n\nSessions were intentionally hands-on, prioritizing experimentation, discussion, and learning through practice rather than prescribing a single \"right\" way to work with AI.\n\n## Findings\n\n- Despite an abundance of online content, many people struggle to navigate AI due to distorted media narratives  \n- Students and practitioners often need permission to experiment, explore, and be wrong more than instruction \n- Hands-on exposure rapidly builds confidence, even for participants with no prior AI experience  \n- Treating AI as a design material rather than a feature or tool resonates strongly with researchers and designers","src/content/projects/01-workshop.md","7bdfb36727bbbe8c",{"html":26,"metadata":27},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>The University of Washington’s MHCI+D program felt it was falling behind in preparing students to work with real-world applications of generative AI. While AI dominated the news cycle, students lacked clear, practical frameworks for understanding how these tools were actually being designed, evaluated, and used in industry.\n\u003Cbr>\u003Cbr>\nBased on my prior sponsorship and mentorship work with the program, as well as my hands-on experience leading generative AI initiatives at Hiya, the university approached me to develop a workshop series to address this gap. I partnered with Sarah Outhwaite, a designer at Google working on generative AI products, to co-create and deliver the curriculum.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Workshop series adopted by the university as a full elective starting in 2026\u003C/li>\n\u003Cli>Strong attendance and sustained student engagement across sessions\u003C/li>\n\u003Cli>Reusable curriculum applied to internal teams at multiple companies\u003C/li>\n\u003Cli>Led to additional speaking engagements and workshops at other universities\u003C/li>\n\u003Cli>Established an ongoing feedback loop with students that refined teaching methods and perspectives on applied AI\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I served as co-instructor and curriculum designer, partnering closely with Sarah Othwaite to define the workshop structure, develop content and exercises, design hands-on activities, and deliver all sessions.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cfigure class=\"float-right\">\n  \u003Cimg src=\"/images/ryan_presenting.jpg\" alt=\"Ryan presenting at UW in front of a slide that reads &#x27;Assessing Risk&#x27;\">\n  \u003Cfigcaption>Presenting during AI Workshops in 2025\u003C/figcaption>\n\u003C/figure>\n\u003Cp>We designed a three-part workshop series, with each session lasting three hours and delivered week over week. Ahead of the workshops, we spent roughly six weeks aligning on perspective and defining a clear foundation for teaching generative AI.\u003C/p>\n\u003Cp>Given how broad and hype-driven the AI landscape had become, we intentionally scoped the curriculum to focus on generative AI as it is actually built and used in practice. We addressed media narratives directly, contrasted them with real adoption patterns, and used historical technology shifts to contextualize how unusually fast AI adoption has been.\u003C/p>\n\u003Cp>Once that foundation was established, the workshops moved into practical application, including designing for AI, using AI within the design process, prompt design, usability testing, risk management, and the limitations of analogy and metaphor when describing AI systems.\u003C/p>\n\u003Cp>Sessions were intentionally hands-on, prioritizing experimentation, discussion, and learning through practice rather than prescribing a single “right” way to work with AI.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Despite an abundance of online content, many people struggle to navigate AI due to distorted media narratives\u003C/li>\n\u003Cli>Students and practitioners often need permission to experiment, explore, and be wrong more than instruction\u003C/li>\n\u003Cli>Hands-on exposure rapidly builds confidence, even for participants with no prior AI experience\u003C/li>\n\u003Cli>Treating AI as a design material rather than a feature or tool resonates strongly with researchers and designers\u003C/li>\n\u003C/ul>",{"headings":28,"localImagePaths":45,"remoteImagePaths":46,"frontmatter":47,"imagePaths":48},[29,33,36,39,42],{"depth":30,"slug":31,"text":32},2,"problem","Problem",{"depth":30,"slug":34,"text":35},"outcomes","Outcomes",{"depth":30,"slug":37,"text":38},"my-role","My Role",{"depth":30,"slug":40,"text":41},"approach","Approach",{"depth":30,"slug":43,"text":44},"findings","Findings",[],[],{"title":14,"summary":15,"role":16,"date":17,"image":18,"imageAlt":19,"imageCaption":20,"featured":21},[],"01-workshop.md","02-machines",{"id":50,"data":52,"body":60,"filePath":61,"digest":62,"rendered":63,"legacyId":76},{"title":53,"summary":54,"role":55,"date":56,"image":57,"imageAlt":58,"imageCaption":59,"featured":21},"How Machines Should Talk","Creating a design system for conversations applicable to new zero-to-one products.","Research & Strategy","2024–2025","/images/machines_talking.png","Four Hiya team members setting around a table and testing conversational voice user experiences","Hiya team members testing and troubleshooting conversational voice experiences","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nHiya saw a clear opportunity to explore AI voice agents. Voice calls are the core of our business, and generative AI presented a path to both solve existing customer problems in the voice channel and expand into adjacent problems we were not yet addressing.\n\u003Cbr>\u003Cbr>\nThis was a completely new space for the company. Not just in terms of new technology, but in understanding *how* AI voice agents should behave. The industry conversation was dominated by demos that optimized for sounding human or appearing impressive, often framed around concepts like the uncanny valley. What was missing was a grounded understanding of what businesses actually want AI agents to sound like, how they want those agents to represent them, and what people calling or using those systems actually value in real interactions.\n\u003Cbr>\u003Cbr>\nWe needed a durable framework that could guide design decisions as the technology rapidly evolved, rather than chasing short-term trends or surface-level feedback.\n\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Created a living conversational design system applicable across teams and products  \n- Successfully launched two zero-to-one products conversational products, Hiya AI Phone and Hiya Voice Agents.\n- Strong qualitative feedback on usability, trust, and engagement with AI voice experiences  \n- Balanced regulatory requirements (such as call recording disclosures across locales) with positive user experience  \n- Built internal confidence that we had the right combination of technologies to deliver the desired experience  \n- Enabled teams to stay focused despite rapid changes in the underlying AI technology landscape  \n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI led the overall strategy and research, framing why conversational design needed to prioritize real user needs over what demos well. I drove all research, synthesis, and decision-making around how AI voice agents should behave and how those principles should be applied across products.\n\n## Approach\n\nThis work spanned a wide range of activities due to the rapidly evolving nature of the technology. We conducted ongoing technical exploration to evaluate available AI voice technologies, continuously testing their strengths, limitations, and changes over time. Because the landscape shifted quickly, this required repeated iteration rather than one-time evaluation.\n\u003Cbr>\u003Cbr>\nI designed surveys and controlled laboratory studies to test different voices, accents, pacing, and latency configurations. In partnership with engineering and product interns working on translation services, we built test environments that allowed native speakers to evaluate multilingual voice interactions and translation quality.\n\u003Cbr>\u003Cbr>\nWe partnered twice with the University of Washington to run applied research on how people want to interact with AI systems. These collaborations included both behavioral research and speculative prototyping to explore future interaction models.\n\u003Cbr>\u003Cbr>\nBeyond structured research, we developed interactive prototypes outside of our core products and, in some cases, deployed them into production to gather real-world feedback. This helped mitigate the artificiality of lab settings, where people often behave differently when they know they are interacting with AI.\n\u003Cbr>\u003Cbr>\nWe also conducted extensive secondary research, drawing from studies on human conversation, cognitive load, turn-taking, interruption, and information retention to better understand constraints like how much information can be communicated effectively during a voice interaction.\n\n## Findings\n\n- What matters most in AI voice interactions is not realism, but clarity, predictability, and trust  \n- Latency and conversational flow have a greater impact on experience than accent or vocal personality  \n- Users value agents that represent them appropriately over agents that attempt to sound human  \n- Providing consistent conversational rules creates better experiences than optimizing each interaction independently  \n- A strong design system allows teams to evaluate feedback in context, distinguish outliers from systemic issues, and resist reacting to every new technology shift","src/content/projects/02-machines.md","b9ad03fd751282e3",{"html":64,"metadata":65},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>Hiya saw a clear opportunity to explore AI voice agents. Voice calls are the core of our business, and generative AI presented a path to both solve existing customer problems in the voice channel and expand into adjacent problems we were not yet addressing.\n\u003Cbr>\u003Cbr>\nThis was a completely new space for the company. Not just in terms of new technology, but in understanding \u003Cem>how\u003C/em> AI voice agents should behave. The industry conversation was dominated by demos that optimized for sounding human or appearing impressive, often framed around concepts like the uncanny valley. What was missing was a grounded understanding of what businesses actually want AI agents to sound like, how they want those agents to represent them, and what people calling or using those systems actually value in real interactions.\n\u003Cbr>\u003Cbr>\nWe needed a durable framework that could guide design decisions as the technology rapidly evolved, rather than chasing short-term trends or surface-level feedback.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Created a living conversational design system applicable across teams and products\u003C/li>\n\u003Cli>Successfully launched two zero-to-one products conversational products, Hiya AI Phone and Hiya Voice Agents.\u003C/li>\n\u003Cli>Strong qualitative feedback on usability, trust, and engagement with AI voice experiences\u003C/li>\n\u003Cli>Balanced regulatory requirements (such as call recording disclosures across locales) with positive user experience\u003C/li>\n\u003Cli>Built internal confidence that we had the right combination of technologies to deliver the desired experience\u003C/li>\n\u003Cli>Enabled teams to stay focused despite rapid changes in the underlying AI technology landscape\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I led the overall strategy and research, framing why conversational design needed to prioritize real user needs over what demos well. I drove all research, synthesis, and decision-making around how AI voice agents should behave and how those principles should be applied across products.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>This work spanned a wide range of activities due to the rapidly evolving nature of the technology. We conducted ongoing technical exploration to evaluate available AI voice technologies, continuously testing their strengths, limitations, and changes over time. Because the landscape shifted quickly, this required repeated iteration rather than one-time evaluation.\n\u003Cbr>\u003Cbr>\nI designed surveys and controlled laboratory studies to test different voices, accents, pacing, and latency configurations. In partnership with engineering and product interns working on translation services, we built test environments that allowed native speakers to evaluate multilingual voice interactions and translation quality.\n\u003Cbr>\u003Cbr>\nWe partnered twice with the University of Washington to run applied research on how people want to interact with AI systems. These collaborations included both behavioral research and speculative prototyping to explore future interaction models.\n\u003Cbr>\u003Cbr>\nBeyond structured research, we developed interactive prototypes outside of our core products and, in some cases, deployed them into production to gather real-world feedback. This helped mitigate the artificiality of lab settings, where people often behave differently when they know they are interacting with AI.\n\u003Cbr>\u003Cbr>\nWe also conducted extensive secondary research, drawing from studies on human conversation, cognitive load, turn-taking, interruption, and information retention to better understand constraints like how much information can be communicated effectively during a voice interaction.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>What matters most in AI voice interactions is not realism, but clarity, predictability, and trust\u003C/li>\n\u003Cli>Latency and conversational flow have a greater impact on experience than accent or vocal personality\u003C/li>\n\u003Cli>Users value agents that represent them appropriately over agents that attempt to sound human\u003C/li>\n\u003Cli>Providing consistent conversational rules creates better experiences than optimizing each interaction independently\u003C/li>\n\u003Cli>A strong design system allows teams to evaluate feedback in context, distinguish outliers from systemic issues, and resist reacting to every new technology shift\u003C/li>\n\u003C/ul>",{"headings":66,"localImagePaths":72,"remoteImagePaths":73,"frontmatter":74,"imagePaths":75},[67,68,69,70,71],{"depth":30,"slug":31,"text":32},{"depth":30,"slug":34,"text":35},{"depth":30,"slug":37,"text":38},{"depth":30,"slug":40,"text":41},{"depth":30,"slug":43,"text":44},[],[],{"title":53,"summary":54,"role":55,"date":56,"image":57,"imageAlt":58,"imageCaption":59,"featured":21},[],"02-machines.md","03-aquistion",{"id":77,"data":79,"body":87,"filePath":88,"digest":89,"rendered":90,"legacyId":103},{"title":80,"summary":81,"role":82,"date":83,"image":84,"imageAlt":85,"imageCaption":86,"featured":21},"10,000ft Acquisition and Integration","Crafting a strategy for integrating 10,000ft and Smartsheet.","Lead Researcher","2019","/images/10kft_smartsheet.png","Screenshot of the 10,000ft console showing the resourcing timeline view","10,000ft resource management interface showing team scheduling and capacity planning","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nAfter four years as an early employee at 10,000ft, the company was acquired by Smartsheet. Smartsheet wanted to integrate the two products immediately, but there was no clear strategy for how the products should work together or what the integration should enable for customers.\n\u003Cbr>\u003Cbr>\nComplicating this further, there was no obvious shared object between the two platforms that made integration straightforward. Smartsheet and 10,000ft were often purchased by the same companies, but rarely used by the same teams—or even the same people. We needed to quickly understand how customers actually used both tools, where integration would add value, and where it could introduce unnecessary complexity.\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Successfully launched the initial 10,000ft–Smartsheet product integration\n- Drived meaningful post-acquisition revenue growth across the broader Smartsheet organization\n- Enabled sales teams to confidently sell resource management alongside work management\n- Clarified how and when customers should use each product together without redundancy\n- Established a shared internal understanding of customer workflows across both platforms\n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI partnered with the product director to define the research strategy, led all research operations and customer discovery, synthesized and shared insights across Smartsheet, worked closely with design to define integration constraints and user needs, and led extensive usability testing prior to launch.\n\n## Approach\n\nWe began by deeply learning the Smartsheet product to understand its mental models, constraints, and value propositions. We then evaluated customer data to identify organizations using both platforms and worked to locate individuals who had exposure to both, which turned out to be relatively rare.\n\u003Cbr>\u003Cbr>\nTo reach these users, I identified geographic concentrations in areas like New York City and San Francisco and organized an in-person research roadshow. At each customer site (primarily design and advertising consultancies)we spent two to three hours with key stakeholders conducting detailed process-mapping sessions. We mapped how projects moved from inception to completion, where Smartsheet and 10,000ft fit into that flow, which tool projects originated in, and where information overlapped or diverged.\n\u003Cbr>\u003Cbr>\nAlongside this, we worked with design and engineering to explore integration concepts and constraints. I partnered closely with the designer, running weekly usability studies to test prototypes and iterate quickly as solutions evolved.\n\u003Cbr>\u003Cbr>\nWe also conducted a broad internal education push, including an all-hands presentation I led for Smartsheet, sharing research findings and framing how customers actually thought about work, resourcing, and integration.\n\n## Findings\n\n- Resource management is closer to casting than staffing—customers prioritize nuanced skill sets and experience over roles or titles\n- Customers sometimes intentionally keep tools out of sync, making control and flexibility more valuable than automation\n- There was no single “right” direction for integration; customers needed the ability to move data both ways depending on where work originated\n- Clear articulation of how two products complement each other is critical for successful cross-selling after an acquisition","src/content/projects/03-aquistion.md","de441aefcf85cd35",{"html":91,"metadata":92},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>After four years as an early employee at 10,000ft, the company was acquired by Smartsheet. Smartsheet wanted to integrate the two products immediately, but there was no clear strategy for how the products should work together or what the integration should enable for customers.\n\u003Cbr>\u003Cbr>\nComplicating this further, there was no obvious shared object between the two platforms that made integration straightforward. Smartsheet and 10,000ft were often purchased by the same companies, but rarely used by the same teams—or even the same people. We needed to quickly understand how customers actually used both tools, where integration would add value, and where it could introduce unnecessary complexity.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Successfully launched the initial 10,000ft–Smartsheet product integration\u003C/li>\n\u003Cli>Drived meaningful post-acquisition revenue growth across the broader Smartsheet organization\u003C/li>\n\u003Cli>Enabled sales teams to confidently sell resource management alongside work management\u003C/li>\n\u003Cli>Clarified how and when customers should use each product together without redundancy\u003C/li>\n\u003Cli>Established a shared internal understanding of customer workflows across both platforms\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I partnered with the product director to define the research strategy, led all research operations and customer discovery, synthesized and shared insights across Smartsheet, worked closely with design to define integration constraints and user needs, and led extensive usability testing prior to launch.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>We began by deeply learning the Smartsheet product to understand its mental models, constraints, and value propositions. We then evaluated customer data to identify organizations using both platforms and worked to locate individuals who had exposure to both, which turned out to be relatively rare.\n\u003Cbr>\u003Cbr>\nTo reach these users, I identified geographic concentrations in areas like New York City and San Francisco and organized an in-person research roadshow. At each customer site (primarily design and advertising consultancies)we spent two to three hours with key stakeholders conducting detailed process-mapping sessions. We mapped how projects moved from inception to completion, where Smartsheet and 10,000ft fit into that flow, which tool projects originated in, and where information overlapped or diverged.\n\u003Cbr>\u003Cbr>\nAlongside this, we worked with design and engineering to explore integration concepts and constraints. I partnered closely with the designer, running weekly usability studies to test prototypes and iterate quickly as solutions evolved.\n\u003Cbr>\u003Cbr>\nWe also conducted a broad internal education push, including an all-hands presentation I led for Smartsheet, sharing research findings and framing how customers actually thought about work, resourcing, and integration.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Resource management is closer to casting than staffing—customers prioritize nuanced skill sets and experience over roles or titles\u003C/li>\n\u003Cli>Customers sometimes intentionally keep tools out of sync, making control and flexibility more valuable than automation\u003C/li>\n\u003Cli>There was no single “right” direction for integration; customers needed the ability to move data both ways depending on where work originated\u003C/li>\n\u003Cli>Clear articulation of how two products complement each other is critical for successful cross-selling after an acquisition\u003C/li>\n\u003C/ul>",{"headings":93,"localImagePaths":99,"remoteImagePaths":100,"frontmatter":101,"imagePaths":102},[94,95,96,97,98],{"depth":30,"slug":31,"text":32},{"depth":30,"slug":34,"text":35},{"depth":30,"slug":37,"text":38},{"depth":30,"slug":40,"text":41},{"depth":30,"slug":43,"text":44},[],[],{"title":80,"summary":81,"role":82,"date":83,"image":84,"imageAlt":85,"imageCaption":86,"featured":21},[],"03-aquistion.md","04-analytics",{"id":104,"data":106,"body":113,"filePath":114,"digest":115,"rendered":116,"legacyId":129},{"title":107,"summary":108,"role":82,"date":109,"image":110,"imageAlt":111,"imageCaption":112,"featured":21},"Redesigning Analytics for Accuracy at Scale","We fixed broken call center analytics through research, analysis, and design before customers felt the impact.","2024","/images/hiya_connect.png","Hiya Connect analytics dashboard showing call center metrics and data visualizations","Redesigned Hiya Connect Dashboard","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nHiya Connect's call center analytics were increasingly unreliable. Different data sources measured calls in incompatible ways, making metrics inconsistent and difficult to interpret. \n\u003Cbr>\u003Cbr>\nAt the same time, new calling features like Apple’s Visual Voicemail were causing traditional metrics like answer rate to appear artificially inflated. \n\u003Cbr>\u003Cbr>\nWe knew customers were making decisions based on data that no longer reflected reality, and upcoming platform changes would only make this worse.\n\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- Delivered a globally consistent analytics framework with no regional caveats\n- Ensured analytics were accurate, comparable, and consistently measured over time\n- Eliminated scenarios where customers would see misleading or unusable data\n- Reduced reliance on fragile third-party data sources and long-term platform risk\n- Launched with minimal customer support issues or post-launch confusion\n\n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI partnered closely with the product manager to define the problem and project plan. I led all discovery, customer research, and statistical analysis, working with data scientists to validate findings. I translated customer needs into design inputs, led usability research on the analytics experience, and partnered with product marketing on customer and internal education around sampling and why this approach was superior to alternatives.\n\n## Approach\n\nWe began by speaking with internal subject matter experts and customers to understand how analytics were actually being used day to day. This work focused on the decisions customers were trying to make, the problems they relied on analytics to solve, and where existing metrics were already falling short. These insights shaped how we approached the redesign from the start.\n\u003Cbr>\u003Cbr>\nFrom there, we audited every available data source to assess coverage, consistency, and long-term viability. I led an evaluation of which sources provided sufficient volume to support statistically meaningful insights across customers.\n\u003Cbr>\u003Cbr>\nPartnering closely with data science, we built a model to determine minimum data thresholds for statistical significance and validated those thresholds against real-world customer data. This led us to a sampling-based analytics approach that balanced accuracy, scalability, and resilience to platform changes.\n\u003Cbr>\u003Cbr>\nI worked with design to ensure the analytics experience never positioned data as more precise than it actually was. When statistical confidence was limited, the product communicated that clearly. We tested these concepts internally and externally, including an independent evaluation with a University of Washington student research team.\n\n## Findings\n\n- Grounding analytics work in real customer decision-making leads to stronger, more resilient systems\n- Accuracy and consistency build trust faster than more metrics or increased complexity\n- Statistical concepts are most effective when available but not forced into the primary experience\n- Early cross-functional alignment and education dramatically reduce launch risk and downstream support needs","src/content/projects/04-analytics.md","eda347c42aab4763",{"html":117,"metadata":118},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>Hiya Connect’s call center analytics were increasingly unreliable. Different data sources measured calls in incompatible ways, making metrics inconsistent and difficult to interpret.\n\u003Cbr>\u003Cbr>\nAt the same time, new calling features like Apple’s Visual Voicemail were causing traditional metrics like answer rate to appear artificially inflated.\n\u003Cbr>\u003Cbr>\nWe knew customers were making decisions based on data that no longer reflected reality, and upcoming platform changes would only make this worse.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>Delivered a globally consistent analytics framework with no regional caveats\u003C/li>\n\u003Cli>Ensured analytics were accurate, comparable, and consistently measured over time\u003C/li>\n\u003Cli>Eliminated scenarios where customers would see misleading or unusable data\u003C/li>\n\u003Cli>Reduced reliance on fragile third-party data sources and long-term platform risk\u003C/li>\n\u003Cli>Launched with minimal customer support issues or post-launch confusion\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I partnered closely with the product manager to define the problem and project plan. I led all discovery, customer research, and statistical analysis, working with data scientists to validate findings. I translated customer needs into design inputs, led usability research on the analytics experience, and partnered with product marketing on customer and internal education around sampling and why this approach was superior to alternatives.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cp>We began by speaking with internal subject matter experts and customers to understand how analytics were actually being used day to day. This work focused on the decisions customers were trying to make, the problems they relied on analytics to solve, and where existing metrics were already falling short. These insights shaped how we approached the redesign from the start.\n\u003Cbr>\u003Cbr>\nFrom there, we audited every available data source to assess coverage, consistency, and long-term viability. I led an evaluation of which sources provided sufficient volume to support statistically meaningful insights across customers.\n\u003Cbr>\u003Cbr>\nPartnering closely with data science, we built a model to determine minimum data thresholds for statistical significance and validated those thresholds against real-world customer data. This led us to a sampling-based analytics approach that balanced accuracy, scalability, and resilience to platform changes.\n\u003Cbr>\u003Cbr>\nI worked with design to ensure the analytics experience never positioned data as more precise than it actually was. When statistical confidence was limited, the product communicated that clearly. We tested these concepts internally and externally, including an independent evaluation with a University of Washington student research team.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>Grounding analytics work in real customer decision-making leads to stronger, more resilient systems\u003C/li>\n\u003Cli>Accuracy and consistency build trust faster than more metrics or increased complexity\u003C/li>\n\u003Cli>Statistical concepts are most effective when available but not forced into the primary experience\u003C/li>\n\u003Cli>Early cross-functional alignment and education dramatically reduce launch risk and downstream support needs\u003C/li>\n\u003C/ul>",{"headings":119,"localImagePaths":125,"remoteImagePaths":126,"frontmatter":127,"imagePaths":128},[120,121,122,123,124],{"depth":30,"slug":31,"text":32},{"depth":30,"slug":34,"text":35},{"depth":30,"slug":37,"text":38},{"depth":30,"slug":40,"text":41},{"depth":30,"slug":43,"text":44},[],[],{"title":107,"summary":108,"role":82,"date":109,"image":110,"imageAlt":111,"imageCaption":112,"featured":21},[],"04-analytics.md","05-selfserve",{"id":130,"data":132,"body":137,"filePath":138,"digest":139,"rendered":140,"legacyId":153},{"title":133,"summary":134,"role":135,"date":17,"image":136,"featured":21},"White Glove to Self-Service","Creating a self-service experience for a historically human-led onboarding process.","Lead Researcher & Strategist","/images/white_glove_self_service.png","\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\n## Problem\n\nHiya Connect was originally built as a sales-led offering. Customers could only access branded call services by going through a human-heavy sales process, and even after a console was created, it was still something customers were introduced to via a traditional sales cycle.\n\nInternally, there was significant skepticism and resistance to the idea of self-service onboarding. Many stakeholders believed the process was “too complex to automate” due to compliance, verification, and configuration steps. Others doubted there was a meaningful audience of customers who would discover, sign up for, and successfully configure branded calling on their own.\n\nWe needed to validate whether a self-service channel was viable, understand what it would need to support, and design an experience that could handle the work previously done by humans—without compromising on compliance or customer success.\n\u003C/div>\n\u003Cdiv class=\"column\">\n\n## Outcomes\n\n- More than doubled first-year sales goals for the self-service channel \n\n- Sales goals for the channel were raised twice by the Chief Product Officer due to strong performance \n\n- More than doubled the number of active accounts within a year \n\n- Significantly reduced customer acquisition cost by enabling low-touch onboarding \n\n- Opened a path for smaller customers to start self-service and grow into larger accounts over time \n\n- Created a larger pool of opted-in leads we could continue to educate and nurture \n\u003C/div>\n\u003C/div>\n\n## My Role\n\nI led all research and strategy to prove that a self-service channel was both feasible and valuable. I mapped the existing end-to-end customer journey and jobs-to-be-done, facilitated cross-functional workshops, partnered with design on the new self-service journey, conducted usability testing with target customers, and designed the analytics needed to track funnel performance and identify friction points after launch.\n\n## Approach\n\n\u003Cfigure class=\"float-right\">\n  \u003Cimg src=\"/images/connect_journey_map.png\" alt=\"A high level view of a detailed customer journey map for Hiya Connect'\" />\n  \u003Cfigcaption>Journey map used to illustrate the pre-self service experience.-\u003C/figcaption>\n\u003C/figure>\n\nWe started by interviewing stakeholders across the existing sales-led journey, speaking with each person involved at every handoff point. This gave us a detailed view of how onboarding actually worked in practice, including the invisible work teams were doing to compensate for system gaps.\n\nIn parallel, we ran our own “tear-down” of the current experience by going through the process as if we were new customers—from initial lead, to qualification, to console access. This helped us reconcile the internal view of the process with the customer perspective and identify gaps, bottlenecks, and redundant steps.\n\nWe then analyzed lead and account data to understand who was already coming to us, which segments were being turned away or stalled in the process, and where there was clear demand that the current model wasn’t serving well.\n\n\u003Cfigure class=\"float-left\">\n  \u003Cimg src=\"/images/sales_funnel_subway.png\" alt=\"A portion of a flow chart in the style of a subway map showing how people move through the website before buying.'\" />\n  \u003Cfigcaption>\"Subway Map\" of different routes customers take on the marketing website before making a purchase\u003C/figcaption>\n\u003C/figure>\n\nUsing this combination of journey mapping and data, we partnered with design to identify which steps were redundant, which could be safely automated, and which required more thoughtful flows or clearer information architecture. This work produced a new end-to-end self-service journey for signing up, configuring, and activating branded calling.\n\nOnce we had a working sign-up flow, we conducted multiple rounds of usability testing—both with the University of Washington and through our own in-house research—focusing on ideal self-service customers. We iterated on copy, flow, and error states to reduce confusion and drop-off.\n\nAt launch, I implemented analytics to track key points in the funnel so we could see where customers slowed down, abandoned, or needed additional support. This instrumentation allowed us to quickly identify and address friction points as real customers moved through the new experience.\n\n## Findings\n\n- There was strong demand for self-service onboarding; we had been effectively turning away qualified customers at the door \n- Many prospective customers did not understand telecom or call center jargon, and needed simpler, more direct language to complete sign-up successfully \n- Compliance and “know your customer” steps created major drop-off risk when they were slow or opaque—speed, transparency, and clear expectations were critical \n- A well-designed self-service flow could maintain a high bar for verification while still feeling fast and respectful of the customer’s time \n- Clear journey mapping and data-backed results helped overcome internal skepticism and shift the organization toward a more scalable, product-led growth motion","src/content/projects/05-selfserve.md","205d8af3906310ce",{"html":141,"metadata":142},"\u003Cdiv class=\"two-column\">\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"problem\">Problem\u003C/h2>\n\u003Cp>Hiya Connect was originally built as a sales-led offering. Customers could only access branded call services by going through a human-heavy sales process, and even after a console was created, it was still something customers were introduced to via a traditional sales cycle.\u003C/p>\n\u003Cp>Internally, there was significant skepticism and resistance to the idea of self-service onboarding. Many stakeholders believed the process was “too complex to automate” due to compliance, verification, and configuration steps. Others doubted there was a meaningful audience of customers who would discover, sign up for, and successfully configure branded calling on their own.\u003C/p>\n\u003Cp>We needed to validate whether a self-service channel was viable, understand what it would need to support, and design an experience that could handle the work previously done by humans—without compromising on compliance or customer success.\u003C/p>\n\u003C/div>\n\u003Cdiv class=\"column\">\n\u003Ch2 id=\"outcomes\">Outcomes\u003C/h2>\n\u003Cul>\n\u003Cli>\n\u003Cp>More than doubled first-year sales goals for the self-service channel\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Sales goals for the channel were raised twice by the Chief Product Officer due to strong performance\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>More than doubled the number of active accounts within a year\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Significantly reduced customer acquisition cost by enabling low-touch onboarding\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Opened a path for smaller customers to start self-service and grow into larger accounts over time\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Created a larger pool of opted-in leads we could continue to educate and nurture\u003C/p>\n\u003C/li>\n\u003C/ul>\n\u003C/div>\n\u003C/div>\n\u003Ch2 id=\"my-role\">My Role\u003C/h2>\n\u003Cp>I led all research and strategy to prove that a self-service channel was both feasible and valuable. I mapped the existing end-to-end customer journey and jobs-to-be-done, facilitated cross-functional workshops, partnered with design on the new self-service journey, conducted usability testing with target customers, and designed the analytics needed to track funnel performance and identify friction points after launch.\u003C/p>\n\u003Ch2 id=\"approach\">Approach\u003C/h2>\n\u003Cfigure class=\"float-right\">\n  \u003Cimg src=\"/images/connect_journey_map.png\" alt=\"A high level view of a detailed customer journey map for Hiya Connect&#x27;\">\n  \u003Cfigcaption>Journey map used to illustrate the pre-self service experience.-\u003C/figcaption>\n\u003C/figure>\n\u003Cp>We started by interviewing stakeholders across the existing sales-led journey, speaking with each person involved at every handoff point. This gave us a detailed view of how onboarding actually worked in practice, including the invisible work teams were doing to compensate for system gaps.\u003C/p>\n\u003Cp>In parallel, we ran our own “tear-down” of the current experience by going through the process as if we were new customers—from initial lead, to qualification, to console access. This helped us reconcile the internal view of the process with the customer perspective and identify gaps, bottlenecks, and redundant steps.\u003C/p>\n\u003Cp>We then analyzed lead and account data to understand who was already coming to us, which segments were being turned away or stalled in the process, and where there was clear demand that the current model wasn’t serving well.\u003C/p>\n\u003Cfigure class=\"float-left\">\n  \u003Cimg src=\"/images/sales_funnel_subway.png\" alt=\"A portion of a flow chart in the style of a subway map showing how people move through the website before buying.&#x27;\">\n  \u003Cfigcaption>\"Subway Map\" of different routes customers take on the marketing website before making a purchase\u003C/figcaption>\n\u003C/figure>\n\u003Cp>Using this combination of journey mapping and data, we partnered with design to identify which steps were redundant, which could be safely automated, and which required more thoughtful flows or clearer information architecture. This work produced a new end-to-end self-service journey for signing up, configuring, and activating branded calling.\u003C/p>\n\u003Cp>Once we had a working sign-up flow, we conducted multiple rounds of usability testing—both with the University of Washington and through our own in-house research—focusing on ideal self-service customers. We iterated on copy, flow, and error states to reduce confusion and drop-off.\u003C/p>\n\u003Cp>At launch, I implemented analytics to track key points in the funnel so we could see where customers slowed down, abandoned, or needed additional support. This instrumentation allowed us to quickly identify and address friction points as real customers moved through the new experience.\u003C/p>\n\u003Ch2 id=\"findings\">Findings\u003C/h2>\n\u003Cul>\n\u003Cli>There was strong demand for self-service onboarding; we had been effectively turning away qualified customers at the door\u003C/li>\n\u003Cli>Many prospective customers did not understand telecom or call center jargon, and needed simpler, more direct language to complete sign-up successfully\u003C/li>\n\u003Cli>Compliance and “know your customer” steps created major drop-off risk when they were slow or opaque—speed, transparency, and clear expectations were critical\u003C/li>\n\u003Cli>A well-designed self-service flow could maintain a high bar for verification while still feeling fast and respectful of the customer’s time\u003C/li>\n\u003Cli>Clear journey mapping and data-backed results helped overcome internal skepticism and shift the organization toward a more scalable, product-led growth motion\u003C/li>\n\u003C/ul>",{"headings":143,"localImagePaths":149,"remoteImagePaths":150,"frontmatter":151,"imagePaths":152},[144,145,146,147,148],{"depth":30,"slug":31,"text":32},{"depth":30,"slug":34,"text":35},{"depth":30,"slug":37,"text":38},{"depth":30,"slug":40,"text":41},{"depth":30,"slug":43,"text":44},[],[],{"title":133,"summary":134,"role":135,"date":17,"image":136,"featured":21},[],"05-selfserve.md"]