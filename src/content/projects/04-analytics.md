---
title: "Redesigning Analytics for Accuracy at Scale"
summary: "We fixed broken call center analytics through research, analysis, and design before customers felt the impact."
role: "Lead Researcher"
date: "2024"
image: "/images/hiya_connect.png"
imageAlt: "Hiya Connect analytics dashboard showing call center metrics and data visualizations"
imageCaption: "Redesigned Hiya Connect Dashboard"
featured: true
---

<div class="two-column">
<div class="column">

## Problem

Hiya Connect's call center analytics were increasingly unreliable. Different data sources measured calls in incompatible ways, making metrics inconsistent and difficult to interpret. 
<br><br>
At the same time, new calling features like Appleâ€™s Visual Voicemail were causing traditional metrics like answer rate to appear artificially inflated. 
<br><br>
We knew customers were making decisions based on data that no longer reflected reality, and upcoming platform changes would only make this worse.

</div>
<div class="column">

## Outcomes

- Delivered a globally consistent analytics framework with no regional caveats
- Ensured analytics were accurate, comparable, and consistently measured over time
- Eliminated scenarios where customers would see misleading or unusable data
- Reduced reliance on fragile third-party data sources and long-term platform risk
- Launched with minimal customer support issues or post-launch confusion

</div>
</div>

## My Role

I partnered closely with the product manager to define the problem and project plan. I led all discovery, customer research, and statistical analysis, working with data scientists to validate findings. I translated customer needs into design inputs, led usability research on the analytics experience, and partnered with product marketing on customer and internal education around sampling and why this approach was superior to alternatives.

## Approach

We began by speaking with internal subject matter experts and customers to understand how analytics were actually being used day to day. This work focused on the decisions customers were trying to make, the problems they relied on analytics to solve, and where existing metrics were already falling short. These insights shaped how we approached the redesign from the start.
<br><br>
From there, we audited every available data source to assess coverage, consistency, and long-term viability. I led an evaluation of which sources provided sufficient volume to support statistically meaningful insights across customers.
<br><br>
Partnering closely with data science, we built a model to determine minimum data thresholds for statistical significance and validated those thresholds against real-world customer data. This led us to a sampling-based analytics approach that balanced accuracy, scalability, and resilience to platform changes.
<br><br>
I worked with design to ensure the analytics experience never positioned data as more precise than it actually was. When statistical confidence was limited, the product communicated that clearly. We tested these concepts internally and externally, including an independent evaluation with a University of Washington student research team.

## Findings

- Grounding analytics work in real customer decision-making leads to stronger, more resilient systems
- Accuracy and consistency build trust faster than more metrics or increased complexity
- Statistical concepts are most effective when available but not forced into the primary experience
- Early cross-functional alignment and education dramatically reduce launch risk and downstream support needs
